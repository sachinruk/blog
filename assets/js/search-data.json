{
  
    
        "post0": {
            "title": "PyTorch Image Patches",
            "content": "Introduction . Getting the 16x16 patches required for the Visual Transformer (ViT) is not that straight forward. This tutorial demonstrates how to use the unfold function in combination with reshape to get the required shape of data. . from typing import List import matplotlib.pyplot as plt from torchvision import io, transforms from torchvision.utils import Image, ImageDraw from torchvision.transforms.functional import to_pil_image %matplotlib inline . Let&#39;s break up our image of size 256 x 256 into 64 x 64 patches. We should end up with 4 rows and 4 columns of these patches. . IMG_SIZE = 256 PATCH_SIZE = 64 resize = transforms.Resize((IMG_SIZE, IMG_SIZE)) img = resize(io.read_image(&quot;../images/autobot.jpg&quot;)) . The actual image looks like so: . to_pil_image(img) . The unfold function can be used to grab a patch of certain size and stride. Unfortunately, you need to use it twice along relevant dimension to get what we are after. . patches = img.unfold(1, PATCH_SIZE, PATCH_SIZE).unfold(2, PATCH_SIZE, PATCH_SIZE) fig, ax = plt.subplots(4, 4) for i in range(4): for j in range(4): sub_img = patches[:, i, j] ax[i][j].imshow(to_pil_image(sub_img)) ax[i][j].axis(&#39;off&#39;) . And finally we can line up the patches and plot them using reshape. . patches = patches.reshape(3, -1, PATCH_SIZE, PATCH_SIZE) patches.transpose_(0, 1) fig, ax = plt.subplots(1, 16, figsize=(12, 12)) for i in range(16): ax[i].imshow(to_pil_image(patches[i])) ax[i].axis(&#39;off&#39;) . Putting it all together . Before sending it through to a transformer, we need to reshape our images from being (batch_size, channels, img_height, img_width) to (batch_size, number_patches, pixels) where pixels in the above example would be 64 x 64 x 3 = 12288 pixels. . Therefore, an example Dataset to read in the images would look like: . from torch.utils.data import Dataset class ImageData(Dataset): def __init__(self, files: List[str]): self.files = files self.resize = transforms.Resize((IMG_SIZE, IMG_SIZE)) self.num_patches = PATCH_SIZE * PATCH_SIZE def __len__(self): return len(self.files) def __getitem__(self, i): img = self.resize(io.read_image(self.files[i])) patches = img .unfold(1, PATCH_SIZE, PATCH_SIZE) .unfold(2, PATCH_SIZE, PATCH_SIZE) patches = patches.reshape(3, -1, PATCH_SIZE, PATCH_SIZE) patches.transpose_(0, 1) return patches.reshape(self.num_patches, -1) . Shameless self promotion . If you enjoyed the tutorial buy me a coffee, or better yet buy my course (usually 90% off). .",
            "url": "https://sachinruk.github.io/blog/pytorch/data/2021/07/03/Image-Patches.html",
            "relUrl": "/pytorch/data/2021/07/03/Image-Patches.html",
            "date": " ‚Ä¢ Jul 3, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Collate function tutorial",
            "content": "Suppose we have the following hypothetical dataset. . class Dataset: def __init__(self): super().__init__() def __len__(self): return 32 def __getitem__(self, idx): return f&quot;hello {idx}&quot;, random.randint(0, 3) rand_ds = Dataset() rand_dl = DataLoader(rand_ds, batch_size=4) . Printing out the first batch, notice how the first element is just a tuple of strings and the second item has automagically been converted into a tensor. . next(iter(rand_dl)) . [(&#39;hello 0&#39;, &#39;hello 1&#39;, &#39;hello 2&#39;, &#39;hello 3&#39;), tensor([0, 1, 1, 0])] . The Collate Function . With the collate function we can convert these strings to a tensor as well. This leads to cleaner code in that data preprocessing is kept away from model code. In my case it actually led to a slightly faster run time per epoch, but I&#39;m not entirely sure why. . The following code takes in a list of size batch size, where each element is a string and it&#39;s corresponding label. Then it parses the strings through the tokenizer, which converts into numerical values thanks to the huggingface tokenizer. But more importantly, note how now you have to convert the y to torch.LongTensor, as otherwise it would remain a tuple. This is certainly an extra step that pytorch was internally taking care of for you. . class CollateFn: def __init__(self): self.tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;) def __call__( self, batch: List[Tuple[str, int]] ) -&gt; Tuple[Dict[str, torch.LongTensor], torch.LongTensor]: x, y = zip(*batch) return self.tokenizer(list(x)), torch.LongTensor(y) . We can add an instance of the above class to our dataloader, which leads us to the following results: . collate_fn = CollateFn() rand_dl = DataLoader(rand_ds, batch_size=4, collate_fn=collate_fn) next(iter(rand_dl)) . . ({&#39;input_ids&#39;: [[101, 19082, 121, 102], [101, 19082, 122, 102], [101, 19082, 123, 102], [101, 19082, 124, 102]], &#39;token_type_ids&#39;: [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], &#39;attention_mask&#39;: [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]}, tensor([2, 1, 3, 1])) . Summary . Collate functions are used to transform data. | You need to transform all outputs, not just simply the one you possibly want. | . Shameless self promotion . If you enjoyed the tutorial buy me a coffee, or better yet buy my course (usually 90% off). .",
            "url": "https://sachinruk.github.io/blog/pytorch/data/2021/06/05/PyTorch-CollateFn.html",
            "relUrl": "/pytorch/data/2021/06/05/PyTorch-CollateFn.html",
            "date": " ‚Ä¢ Jun 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "The Annotated TabNet",
            "content": ". Introduction . We are talking about TabNet today which is a network designed for Tabular data. One aspect that tree based models such as Random Forest (RF) and XgBoost can claim over Neural Nets is the explainability of the model. . Personally, one of the coolest features of this network is the ability for the network to point out which features were used in it&#39;s final decision. And these features change from input to input. We have a visual demo using MNIST below. RF does not have this kind of flexibility and is only one static graph for any kind of input. . Acknowledgement . Kudos to Paul Tune for pointing out how to configure the loss function in keras. In terms of the actual implementation it was done very much thanks to this YouTube talk done by Sebastien Fischman and hosted by Abhishek Thakur. Much of the TensorFlow code was very much a reimplimentation of the PyTorch code. . Model . This section will outline the blocks that we have used in creating the overall model. See the diagram above to refer to the sections mentioned below. We will not be talking about part (b) of the diagram as that from my understanding is only used for completing missing values. . Fully-Connected Block . Let&#39;s look at section (c) of the diagram and especially one set of the blue-green-red boxes and call that one Fully-Connected layer. Instead of ReLU the authors have used a GLU activation (the calculation of which is shown in the first two lines). The FCBlock will form as one of the building blocks of the entire architecture. . def GLU(x): return x * tf.sigmoid(x) class FCBlock(layers.Layer): def __init__(self, units): super().__init__() self.layer = layers.Dense(units) self.bn = layers.BatchNormalization() def call(self, x): return GLU(self.bn(self.layer(x))) . Feature Transformer . Next bit of the feature transformer in section (c) is the Shared Block and the Decision Block which look extrememly similar. This is why I sub-classed DecisionBlock to have parent SharedBlock. Also worth mentioning that this is all possible thanks to the new features of TensorFlow 2.0+. . The shared block is simply stacking two FCBlocks with a residual connection which is multiplied by sqrt(0.5). I&#39;m not sure if this multiplication is necessary but I will leave it in. . The decision block is almost the same except that there are two residual connections. . Now in terms of functionality, refer to section (a) of the diagram. The user needs to define the number of steps as one of the hyper-parameters. The feature transformer is repeated number of steps + 1 many times. In this, the SharedBlock weights are shared across all of that, whereas the DecisionBlock weights are different for each step. . class SharedBlock(layers.Layer): def __init__(self, units, mult=tf.sqrt(0.5)): super().__init__() self.layer1 = FCBlock(units) self.layer2 = FCBlock(units) self.mult = mult def call(self, x): out1 = self.layer1(x) out2 = self.layer2(out1) return out2 + self.mult * out1 class DecisionBlock(SharedBlock): def __init__(self, units, mult=tf.sqrt(0.5)): super().__init__(units, mult) def call(self, x): out1 = x * self.mult + self.layer1(x) out2 = out1 * self.mult + self.layer2(out1) return out2 . Attentive Transformer . This is the part where the magic happens. The attentive transformer decides which bits of the input features (x) it needs to pay attention (mask) at each step. . Before, talking about the Attentive transformer we need to talk about the split module and the prior layers. The split module simply splits the output of the feature transformer into two portions. One portion which feeds into the attentive transformer, and the other which goes into the output of our overall network. The portions (n_d, n_a) are hyper-parameters that the user needs to specify and it would sum to the number of output nodes of the decision layer. . The attentive layer takes the n_a output nodes from the decision block, runs it through a dense layer and batch norm layer before passing through a sparsemax layer. Sparsemax is similar to softmax in that the output sums to one. However, it drives some of the smaller outputs to exactly zero unlike softmax (which can only get close to zero). It is important to note that the dense layer used here projects n_a nodes to the same dimensionality as the input features, as we will use this as a mask to select features. . The prior layer is used as a tool to suppress some of the inputs that were used before in a previous step. In the first step none of the input had been used before, and therefore the output of the attentive transformer is unaffected by the prior. However, in the second step (and onwards), the prior is updated to be prior = previous_prior * (constant - previous_mask). The constant mentioned here is a number close to one. Therefore, if the previous_mask output by the attentive mask had numbers close to one, the prior is pushed closer to zero, whereas the unused inputs having a mask close to zero, would have a prior closer to one. The priors closer to zero would effectively act as a suppressant before passing into the sparsemax activation in the attentive layer. . The reason that we attempt to use this kind of prior is so that each step attempts to find unique features which adds to the output of the network. . class Prior(layers.Layer): def __init__(self, gamma=1.1): super().__init__() self.gamma = gamma def reset(self): self.P = 1.0 def call(self, mask): self.P = self.P * (self.gamma - mask) return self.P . class AttentiveTransformer(layers.Layer): def __init__(self, units): super().__init__() self.layer = layers.Dense(units) self.bn = layers.BatchNormalization() def call(self, x, prior): return sparsemax(prior * self.bn(self.layer(x))) . Putting it All Together . Much of the architecture has already been discussed in the two previous sections. However, there is two more missing pieces. . Firstly, the loss function. In additional to the loss function that is used for the task (CrossEntropy in the case of MNIST classification), there is an additional loss on the mask values to drive the values towards either 0 or 1. The (entropy) loss is defined as follows: $$ begin{align} L_{mask} = - M log (M + epsilon) end{align} $$ The plot below shows entropy as a function of one mask value. We average out all available mask entropy values to get final loss. Note how they are minimised at either extreme. . The second part that we have not discussed so far is the actual output of the model. The n_d number of inputs that do not go through the attentive layer gets passed through a ReLU activation at each step before being added up for the final output. This is compared against a target by using a task specific loss function (cross-entropy in our case). . One other thing I&#39;d like to bring your attention to (see what I did there?) is the fact that I used a for loop inside the call function in the Module below. It might seem like a minor thing, but this kind of flexibility in building a model was not available before to my knowledge. . Mask Importance . The whole point of building this model was to be able to select features and to be able to explain the model. The way that we calculate feature importance is by calculating the importance of each step, and by multiplying that by the mask. . Step importance is calculated as the sum of the (n_d) outputs of the feature transformer. . class TabNet(keras.Model): def __init__(self, input_dim, output_dim, steps, n_d, n_a, gamma=1.3): super().__init__() # hyper-parameters self.n_d, self.n_a, self.steps = n_d, n_a, steps # input-normalisation self.bn = layers.BatchNormalization() # Feature Transformer self.shared = SharedBlock(n_d+n_a) self.first_block = DecisionBlock(n_d+n_a) self.decision_blocks = [DecisionBlock(n_d+n_a)] * steps # Attentive Transformer self.attention = [AttentiveTransformer(input_dim)] * steps self.prior_scale = Prior(gamma) # final layer self.final = layers.Dense(output_dim) self.eps = 1e-8 self.add_layer = layers.Add() @tf.function def call(self, x): self.prior_scale.reset() final_outs = [] mask_losses = [] x = self.bn(x) attention = self.first_block(self.shared(x))[:,:self.n_a] for i in range(self.steps): mask = self.attention[i](attention, self.prior_scale.P) entropy = mask * tf.math.log(mask + self.eps) mask_losses.append( -tf.reduce_sum(entropy, axis=-1) / self.steps ) prior = self.prior_scale(mask) out = self.decision_blocks[i](self.shared(x * prior)) attention, output = out[:,:self.n_a], out[:,self.n_a:] final_outs.append(tf.nn.relu(output)) final_out = self.add_layer(final_outs) mask_loss = self.add_layer(mask_losses) return self.final(final_out), mask_loss def mask_importance(self, x): self.prior_scale.reset() feature_importance = 0 x = self.bn(x) attention = self.first_block(self.shared(x))[:,:self.n_a] for i in range(self.steps): mask = self.attention[i](attention, self.prior_scale.P) prior = self.prior_scale(mask) out = self.decision_blocks[i](self.shared(x * prior)) attention, output = out[:,:self.n_a], out[:,self.n_a:] step_importance = tf.reduce_sum(tf.nn.relu(output), axis=1, keepdims=True) feature_importance += mask * step_importance return feature_importance . . Loss and Fitting . Now this is the part that I thought was strange in the keras API, unless I&#39;m doing something wrong. Looking at the TabNet class above, it returns the actual prediction, and the mask_loss. From my understanding of loss functions in keras, it is always the &quot;true y&quot; and the &quot;predictions&quot; over which we need to define a loss. . As shown below my initial attempt was to unpack the predictions into logits and mask losses, and then in the next line to take the cross entropy and average out the mask loss. This however resulted in an error. . What did work however, was to define mask_loss function defined below which ignores the y_true input, and to add the loss_weights during model.compile. It seems like attempting to add a breakpoint and debug does not seem to work after you run model.compile. . from keras.losses import SparseCategoricalCrossentropy sce = SparseCategoricalCrossentropy(from_logits=True) reg_sparse = 0.01 def full_loss(y_true, y_pred): logits, mask_loss = y_pred return sce(y_true, logits) + reg_sparse * mask_loss.mean() . . def mask_loss(y_true, mask_losses): return tf.reduce_mean(mask_losses) model = TabNet(784, 10, 2, 10, 10, 1.3) model.compile( &#39;Adam&#39;, loss=[sce, mask_loss], loss_weights=[1, 0.01] ) model.fit( x_train, y_train, epochs=5, batch_size=256, validation_split=0.15, workers=mp.cpu_count() ) . . Epoch 1/5 200/200 [==============================] - 24s 96ms/step - loss: 1.7993 - output_1_loss: 1.7842 - output_2_loss: 1.5173 - val_loss: 0.6537 - val_output_1_loss: 0.6461 - val_output_2_loss: 0.7665 Epoch 2/5 200/200 [==============================] - 19s 94ms/step - loss: 0.5893 - output_1_loss: 0.5828 - output_2_loss: 0.6446 - val_loss: 0.2964 - val_output_1_loss: 0.2923 - val_output_2_loss: 0.4080 Epoch 3/5 200/200 [==============================] - 19s 94ms/step - loss: 0.2887 - output_1_loss: 0.2849 - output_2_loss: 0.3869 - val_loss: 0.2182 - val_output_1_loss: 0.2151 - val_output_2_loss: 0.3119 Epoch 4/5 200/200 [==============================] - 19s 95ms/step - loss: 0.2148 - output_1_loss: 0.2118 - output_2_loss: 0.3055 - val_loss: 0.1908 - val_output_1_loss: 0.1882 - val_output_2_loss: 0.2625 Epoch 5/5 200/200 [==============================] - 19s 94ms/step - loss: 0.1764 - output_1_loss: 0.1737 - output_2_loss: 0.2686 - val_loss: 0.1775 - val_output_1_loss: 0.1751 - val_output_2_loss: 0.2383 . &lt;tensorflow.python.keras.callbacks.History at 0x7f6402c12510&gt; . Results . Model has 91.8% accuracy on test set. See below for calculation. . y_pred = model.predict(x_test)[0].argmax(axis=-1) (y_pred == y_test).mean() . . 0.918 . Let&#39;s see what features got picked up for the first ten images: . As can be seen in most images the important features on the actual digit itself. However, with the first two images above there are no important features in the actual digit. . It is worth noting that the MNIST digit is not a binary image and even in the far corners the pixel values may not be exactly zero. Alternatively, it is also fathomable to think that the net is making sure that the pixels are zero in certain parts of the image to ensure that the digit is of a certain shape. . Conclusion . Hopefully that demystified TabNets. I personally feel like the sparsemax activation is not explored enough in other areas of DL and probably has more to contribute in the coming years. . In terms of TabNet, it&#39;s great to see explainability being the pure focus of a paper. The experiments conducted in the paper claims that it has beaten XgBoost in some benchmarks and hopefully TabNet will be the gold standard in this space. . Finally, it was really good to play with Tensorflow 2 in a familiar environment to pytorch users. . Shameless Self Promotion . See here for my course on Machine Learning and Deep Learning (Use code DEEPSCHOOL-MARCH to 85% off). .",
            "url": "https://sachinruk.github.io/blog/tensorflow/2021/04/05/Tabnet_From_Scratch.html",
            "relUrl": "/tensorflow/2021/04/05/Tabnet_From_Scratch.html",
            "date": " ‚Ä¢ Apr 5, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Multilingual CLIP with Huggingface + PyTorch Lightning ü§ó ‚ö°",
            "content": ". This is a walkthrough of training CLIP by OpenAI. CLIP was designed to put both images and text into a new projected space such that they can map to each other by simply looking at dot products. . Traditionally training sets like imagenet only allowed you to map images to a single class (and hence one word). This method allows you to map text to images, but can also be used to map images to text if the need arises. . This particular blog however is specifically how we managed to train this on colab GPUs using huggingface transformers and pytorch lightning. . Thanks to fastpages by fastai you can run this blog on colab using GPUS. . Acknowledgement . Kudos to the following CLIP tutorial in the keras documentation. . The important thing to notice about the constants is the embedding dim. We will project the output of a resnet and transformers into 512 dimensional space. . EMBED_DIM = 512 TRANSFORMER_EMBED_DIM = 768 MAX_LEN = 32 # Maximum length of text TEXT_MODEL = &quot;distilbert-base-multilingual-cased&quot; EPOCHS = 5 BATCH_SIZE = 64 . Data . We download the coco dataset which contains 5 captions per image and has roughly 82k images. We take 20% of it to be our validation set. . Considering that the image backbone is trained using imagenet, we normalise it using the imagenet stats as shown in the transforms normalize step. We also resize the image to 128x128 to make sure it trains in reasonable time. . Warning: Downloading the files will take a while (~5-10 minutes). . class Tokenizer: def __init__(self, tokenizer: BertTokenizer) -&gt; None: self.tokenizer = tokenizer def __call__(self, x: str) -&gt; AutoTokenizer: return self.tokenizer( x, max_length=MAX_LEN, truncation=True, padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot; ) def decode(self, x: Dict[str, torch.LongTensor]): return [self.tokenizer.decode(sentence[:sentence_len]) for sentence, sentence_len in zip(x[&quot;input_ids&quot;], target[&quot;attention_mask&quot;].sum(axis=-1))] tokenizer = Tokenizer(AutoTokenizer.from_pretrained(TEXT_MODEL)) . img = inv_tfm(img) plt.imshow(np.rot90(img.transpose(0, 2), 3)) plt.title(tokenizer.decode(target)[0]) plt.show() . train_len = int(0.8*len(cap)) train_data, valid_data = random_split(cap, [train_len, len(cap) - train_len]) train_dl = DataLoader(train_data, BATCH_SIZE, pin_memory=True, shuffle=True, num_workers=4, drop_last=True) valid_dl = DataLoader(valid_data, BATCH_SIZE, pin_memory=True, shuffle=False, num_workers=4, drop_last=False) . /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) . Model . There are two main models, the VisionEncoder and the TextEncoder which have resnet18 and distilbert as backbones. In order to make it multi-lingual, we simply choose the distilbert-multilingual model and that&#39;s it! No need to specifically train on non-english words as you will soon see. . The Projection module, takes the embeddings from vision and text encoders and projects them into 512 dimensional space. . Two things to note: . We have frozen both the text and vision encoder backbones and do not retrain their weights at all. | For both encoders the final output is normalised to be of unit length. | class Projection(nn.Module): def __init__(self, d_in: int, d_out: int, p: float=0.5) -&gt; None: super().__init__() self.linear1 = nn.Linear(d_in, d_out, bias=False) self.linear2 = nn.Linear(d_out, d_out, bias=False) self.layer_norm = nn.LayerNorm(d_out) self.drop = nn.Dropout(p) def forward(self, x: torch.Tensor) -&gt; torch.Tensor: embed1 = self.linear1(x) embed2 = self.drop(self.linear2(F.gelu(embed1))) embeds = self.layer_norm(embed1 + embed2) return embeds . class VisionEncoder(nn.Module): def __init__(self, d_out: int) -&gt; None: super().__init__() base = models.resnet34(pretrained=True) d_in = base.fc.in_features base.fc = nn.Identity() self.base = base self.projection = Projection(d_in, d_out) for p in self.base.parameters(): p.requires_grad = False def forward(self, x): projected_vec = self.projection(self.base(x)) projection_len = torch.norm(projected_vec, dim=-1, keepdim=True) return projected_vec / projection_len . class TextEncoder(nn.Module): def __init__(self, d_out: int) -&gt; None: super().__init__() self.base = AutoModel.from_pretrained(TEXT_MODEL) self.projection = Projection(TRANSFORMER_EMBED_DIM, d_out) for p in self.base.parameters(): p.requires_grad = False def forward(self, x): out = self.base(**x)[0] out = out[:, 0, :] # get CLS token output projected_vec = self.projection(out) projection_len = torch.norm(projected_vec, dim=-1, keepdim=True) return projected_vec / projection_len . CLIP loss function . For someone like me who hasn&#39;t played around with contrastive loss, this was the most interesting part. . We know that we want the vectors of the corresponding image and the text to line up. Which means that the dot product has to be as close to one as possible. For everything else we need to push it towards 0. . Therfore for a given caption, we take the softmax of the dot products across all images, and then take cross entropy loss. Similarly for a given image, we repeat the process across all captions. We average these two losses. . In terms of which element is the true positive within a batch, remember that we are sending image, caption pairs already lined up. Therefore we want all the diagonal elements to line up while all off-diagonal elements we want to push towards zero. . def contrastive_loss(logits, dim): neg_ce = torch.diag(F.log_softmax(logits, dim=dim)) return -neg_ce.mean() def clip_loss(similarity: torch.Tensor) -&gt; torch.Tensor: caption_loss = contrastive_loss(similarity, dim=0) image_loss = contrastive_loss(similarity, dim=1) return (caption_loss + image_loss) / 2.0 def metrics(similarity: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]: y = torch.arange(len(similarity)).to(similarity.device) img2cap_match_idx = similarity.argmax(dim=1) cap2img_match_idx = similarity.argmax(dim=0) img_acc = (img2cap_match_idx == y).float().mean() cap_acc = (cap2img_match_idx == y).float().mean() return img_acc, cap_acc . Model . If you haven&#39;t used pytorch lightning before, the benefit is that you do not need to stress about which device to put it in, remembering to zero the optimizer etc. All of that is taken care of. Just simply specify the training and validation steps, along with the optimizer and you are good to go. . The other benefit that I really like is logging. You just need to write self.log(&quot;name&quot;, metric_to_track) and it will log to tensorboard by default, or any other kind of logger for that matter. . class Model(pl.LightningModule): def __init__(self, lr: float = 1e-3 ) -&gt; None: super().__init__() self.vision_encoder = VisionEncoder(EMBED_DIM) self.caption_encoder = TextEncoder(EMBED_DIM) # self.tokenizer = Tokenizer(AutoTokenizer.from_pretrained(TEXT_MODEL)) self.lr = lr def common_step(self, batch: Tuple[torch.Tensor, List[str]]) -&gt; torch.Tensor: images, text = batch # device = images.device # text_dev = {k: v.to(self.device) for k, v in text.items()} breakpoint() image_embed = self.vision_encoder(images) caption_embed = self.caption_encoder(text) similarity = caption_embed @ image_embed.T loss = clip_loss(similarity) img_acc, cap_acc = metrics(similarity) return loss, img_acc, cap_acc def training_step( self, batch: Tuple[torch.Tensor, List[str]], *args: list ) -&gt; torch.Tensor: loss, img_acc, cap_acc = self.common_step(batch) self.log(&quot;training_loss&quot;, loss, on_step=True) self.log(&quot;training_img_acc&quot;, img_acc, on_step=True, prog_bar=True) self.log(&quot;training_cap_acc&quot;, cap_acc, on_step=True, prog_bar=True) return loss def validation_step( self, batch: Tuple[torch.Tensor, List[str]], *args: list ) -&gt; torch.Tensor: loss, img_acc, cap_acc = self.common_step(batch) self.log(&quot;validation_loss&quot;, loss, on_step=True) self.log(&quot;validation_img_acc&quot;, img_acc, on_step=True, prog_bar=True) self.log(&quot;validation_cap_acc&quot;, cap_acc, on_step=True, prog_bar=True) return loss def configure_optimizers(self) -&gt; torch.optim.Optimizer: vision_params = {&quot;params&quot;: self.vision_encoder.projection.parameters(), &quot;lr&quot;: self.lr} caption_params = {&quot;params&quot;: self.caption_encoder.projection.parameters() , &quot;lr&quot;: self.lr} return torch.optim.Adam([vision_params, caption_params]) . Train . Training is straight forward as show in the five lines below. Using 16 bit precision almost halved the training time from 16 minutes to 9 minutes per epoch. Notice how easy it was to add half precision training and gradient clipping. . Also one thing to note is that I could not get this working on TPUs so if anyone knows what I need to adjust, please let me know. Setting tpu_cores=8 just did not work. . model = Model(1e-3) trainer = pl.Trainer( max_epochs= 1, # gpus=torch.cuda.device_count(), tpu_cores=1, gradient_clip_val=1.0, precision=16 ) trainer.fit(model, train_dl, valid_dl) # . GPU available: False, used: False TPU available: True, using: 1 TPU cores . RuntimeError Traceback (most recent call last) &lt;ipython-input-54-34cb47015c5f&gt; in &lt;module&gt;() 7 precision=16 8 ) -&gt; 9 trainer.fit(model, train_dl, valid_dl) # /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule) 497 498 # dispath `start_training` or `start_testing` or `start_predicting` --&gt; 499 self.dispatch() 500 501 # plugin will finalized fitting (e.g. ddp_spawn will load trained model) /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in dispatch(self) 544 545 else: --&gt; 546 self.accelerator.start_training(self) 547 548 def train_or_test_or_predict(self): /usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer) 71 72 def start_training(self, trainer): &gt; 73 self.training_type_plugin.start_training(trainer) 74 75 def start_testing(self, trainer): /usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/tpu_spawn.py in start_training(self, trainer) 183 del os.environ[&#34;XLA_USE_BF16&#34;] 184 self._close_logger(trainer) --&gt; 185 xmp.spawn(self.new_process, **self.xmp_spawn_kwargs) 186 187 def start_testing(self, trainer) -&gt; None: /usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py in spawn(fn, args, nprocs, join, daemon, start_method) 384 pf_cfg = _pre_fork_setup(nprocs) 385 if pf_cfg.num_devices == 1: --&gt; 386 _start_fn(0, pf_cfg, fn, args) 387 else: 388 return torch.multiprocessing.start_processes( /usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py in _start_fn(index, pf_cfg, fn, args) 321 # environment must be fully setup before doing so. 322 _setup_replication() --&gt; 323 fn(gindex, *args) 324 325 /usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/tpu_spawn.py in new_process(self, process_idx, trainer, mp_queue) 84 trainer.progress_bar_callback.disable() 85 &gt; 86 self.model_to_device() 87 trainer.accelerator.setup_optimizers(trainer) 88 trainer.precision_plugin.connect(self._model, None, None) /usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/tpu_spawn.py in model_to_device(self) 104 105 def model_to_device(self) -&gt; None: --&gt; 106 self._model.to(xm.xla_device()) 107 108 def barrier(self, name: Optional[str] = None) -&gt; None: /usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/decorators.py in inner_fn(self, *args, **kwargs) 87 def inner_fn(self, *args, **kwargs): 88 pre_layer_count = len(list(self.parameters())) &gt; 89 module = fn(self, *args, **kwargs) 90 self.on_post_move_to_device() 91 post_layer_count = len(list(self.parameters())) /usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/device_dtype_mixin.py in to(self, *args, **kwargs) 118 out = torch._C._nn._parse_to(*args, **kwargs) 119 self.__update_properties(device=out[0], dtype=out[1]) --&gt; 120 return super().to(*args, **kwargs) 121 122 def cuda(self, device: Optional[int] = None) -&gt; Module: /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in to(self, *args, **kwargs) 814 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking) 815 --&gt; 816 return self._apply(convert) 817 818 def register_backward_hook( /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _apply(self, fn) 528 def _apply(self, fn): 529 for module in self.children(): --&gt; 530 module._apply(fn) 531 532 def compute_should_use_set_data(tensor, tensor_applied): /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _apply(self, fn) 528 def _apply(self, fn): 529 for module in self.children(): --&gt; 530 module._apply(fn) 531 532 def compute_should_use_set_data(tensor, tensor_applied): /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _apply(self, fn) 528 def _apply(self, fn): 529 for module in self.children(): --&gt; 530 module._apply(fn) 531 532 def compute_should_use_set_data(tensor, tensor_applied): /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _apply(self, fn) 528 def _apply(self, fn): 529 for module in self.children(): --&gt; 530 module._apply(fn) 531 532 def compute_should_use_set_data(tensor, tensor_applied): /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _apply(self, fn) 550 # `with torch.no_grad():` 551 with torch.no_grad(): --&gt; 552 param_applied = fn(param) 553 should_use_set_data = compute_should_use_set_data(param, param_applied) 554 if should_use_set_data: /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in convert(t) 812 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, 813 non_blocking, memory_format=convert_to_format) --&gt; 814 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking) 815 816 return self._apply(convert) /usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/signal_handling.py in handler(signum, frame) 64 # This following call uses `waitid` with WNOHANG from C side. Therefore, 65 # Python can still get and update the process status successfully. &gt; 66 _error_if_any_worker_fails() 67 if previous_handler is not None: 68 assert callable(previous_handler) RuntimeError: DataLoader worker (pid 6326) is killed by signal: Killed. . Run the following cell if you wish to see the logs in tensorboard. But here&#39;s a screenshot I took: . %reload_ext tensorboard %tensorboard --logdir ./lightning_logs/ . Results . I will compare the text embeddings of the first batch (in the validation set) to all the images of the validation set by taking the dot product between them. . similarity = caption_embed @ image_embed.T val, closest = similarity.topk(5, dim=-1) similarity.shape . torch.Size([64, 16557]) . draw_result(i, similarity_matrix) is a convenience function that takes the i-th caption and the similarity matrix, and plots the five closest images, along with the true image. The similarity between the caption and the image is shown in the title. The caption is printed first. . The histogram show the similarity of the caption to all images as a histogram. . draw_result(2, similarity) . A baseball player in the outfield with his hands up, standing next to a team mascot. . draw_result(1, similarity) . A watch and clock repair shop window with clocks on display. . draw_result(10, similarity) . A person on a skateboard on the ground. . Below is the google translted version of one of the captions. . English caption: &quot;A zebra standing up with it&#39;s head down and eating grass on the dirt ground.&quot;, tranlated into Spanish: . text = &quot;Una cebra de pie con la cabeza gacha y comiendo hierba en el suelo de tierra.&quot; text_dev = {k: v.to(device) for k, v in tokenizer(text).items()} with torch.no_grad(): caption_embed_text = caption_encoder(text_dev) similarity_text = caption_embed_text @ image_embed.T . draw_result_single_query(10, similarity_text) . Skateboarder conducting a trick with bicycles in the background. . Again a translated version, this time to french. English caption: &quot;A laptop is displayed on a small wooden platform.&quot; . text = &quot;Un ordinateur portable est affich√© sur une petite plate-forme en bois.&quot; text_dev = {k: v.to(device) for k, v in tokenizer(text).items()} with torch.no_grad(): caption_embed_text = caption_encoder(text_dev) similarity_text = caption_embed_text @ image_embed.T draw_result_single_query(3, similarity_text) . Laptop computer on a small table on the side of a bed . The russian translation below is doing terrible though, so its clearly not bullet proof. Or perhaps I need to train for a bit longer. English caption: &quot;A shop filled with different kinds of clocks. . text = &quot;–ú–∞–≥–∞–∑–∏–Ω —Å —Ä–∞–∑–Ω—ã–º–∏ —á–∞—Å–∞–º–∏&quot; text_dev = {k: v.to(device) for k, v in tokenizer(text).items()} with torch.no_grad(): caption_embed_text = caption_encoder(text_dev) similarity_text = caption_embed_text @ image_embed.T draw_result_single_query(1, similarity_text) . A room filled with clocks through a window. . And lastly I check a single word version. Notice how the dog does kind of look like a bear. Maybe it&#39;s name is bear? . text = &quot;bear&quot; text_dev = {k: v.to(device) for k, v in tokenizer(text).items()} with torch.no_grad(): caption_embed_text = caption_encoder(text_dev) similarity_text = caption_embed_text @ image_embed.T draw_result_single_query(1, similarity_text) . Large collection of digital and analog clocks on display. . Would love to hear any thoughts and comments on the above. . Shameless Self Promotion . See here for my course on Machine Learning and Deep Learning (Use code DEEPSCHOOL-MARCH to 85% off). .",
            "url": "https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html",
            "relUrl": "/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html",
            "date": " ‚Ä¢ Mar 7, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Tensorflow Learning Rate Finder",
            "content": "The following tutorial shows how to implement a learning rate finder from scratch, using Keras callbacks. . But first a quick refresher on how we would do model fitting on a simple network: . Imports and Data . import matplotlib.pyplot as plt import numpy as np import tensorflow as tf from tensorflow import keras from tensorflow.keras.losses import SparseCategoricalCrossentropy %matplotlib inline . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() x_train = x_train.reshape(len(x_train), -1) x_test = x_test.reshape(len(x_test), -1) # Rescale the images from [0,255] to the [0.0,1.0] range. x_train, x_test = x_train/255.0, x_test/255.0 . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step . Model . model = keras.Sequential() model.add(keras.layers.Input(x_train.shape[-1])) model.add(keras.layers.Dense(100, activation=&quot;relu&quot;)) model.add(keras.layers.Dense(10, activation=&quot;softmax&quot;)) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;]) . model.fit(x_train, y_train, batch_size=64, epochs=5) . Epoch 1/5 938/938 [==============================] - 3s 3ms/step - loss: 0.5480 - accuracy: 0.8500 Epoch 2/5 938/938 [==============================] - 3s 3ms/step - loss: 0.1601 - accuracy: 0.9546 Epoch 3/5 938/938 [==============================] - 3s 3ms/step - loss: 0.1106 - accuracy: 0.9681 Epoch 4/5 938/938 [==============================] - 2s 3ms/step - loss: 0.0817 - accuracy: 0.9773 Epoch 5/5 938/938 [==============================] - 2s 3ms/step - loss: 0.0632 - accuracy: 0.9811 . &lt;tensorflow.python.keras.callbacks.History at 0x7f60660ca0f0&gt; . LR Finder . Let me outline the logic behind LR finder before we dive into the code. The basic idea is to vary the learning rate and note down the loss. At a certain point when the learning rate is too high the loss will start increasing again. . Therefore the tasks that we have to do in order are: . Get the minimum and maximum learning rate we are willing to look at. | Initialise buffers to hold the learning rate and losses. | Before we begin this process, get the current model weights so we can restore it later. | Get a batch, and get the loss for that batch, and increase the learning rate. | Repeat the above step until maximum learning rate is reached. | Reset old weights to model. | Plot the model. | The above 7 steps can be seen in the LRFind class below. on_train_begin, on_train_batch_end, on_train_end are simply callback functions provided by the keras API. Hopefully, they are self explanatory. . class LRFind(tf.keras.callbacks.Callback): def __init__(self, min_lr, max_lr, n_rounds): self.min_lr = min_lr self.max_lr = max_lr self.step_up = (max_lr / min_lr) ** (1 / n_rounds) self.lrs = [] self.losses = [] def on_train_begin(self, logs=None): self.weights = self.model.get_weights() self.model.optimizer.lr = self.min_lr def on_train_batch_end(self, batch, logs=None): self.lrs.append(self.model.optimizer.lr.numpy()) self.losses.append(logs[&quot;loss&quot;]) self.model.optimizer.lr = self.model.optimizer.lr * self.step_up if self.model.optimizer.lr &gt; self.max_lr: self.model.stop_training = True def on_train_end(self, logs=None): self.model.set_weights(self.weights) . We want to reset the model since it already learnt something decent above, but feel free to skip the next cell to see if results differ. . model = keras.Sequential() model.add(keras.layers.Input(x_train.shape[-1])) model.add(keras.layers.Dense(100, activation=&quot;relu&quot;)) model.add(keras.layers.Dense(10, activation=&quot;softmax&quot;)) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;]) . Before we go ahead and run learning rate finder, a few things we should define. . First, we need to use tf.data.Dataset.from_tensor_slices incase there aren&#39;t enough batches per epoch for learning rate to go from min_lr to max_lr. | We use EPOCHS=1 but, this is a repeating dataset forever as seen in line 6 below. It is lr_finder_steps that force this repetition to stop at 400 batches. | Instead of model.fit(x_train, y_train,...), we use model.fit(train_dataset). | When plotting we use the log scale since we increase learning rate multiplicatively. | . EPOCHS = 1 BATCH_SIZE = 64 lr_finder_steps = 400 lr_find = LRFind(1e-6, 1e1, lr_finder_steps) train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) .repeat() .shuffle(2048) .batch(BATCH_SIZE) model.fit( train_dataset, steps_per_epoch=lr_finder_steps, epochs=EPOCHS, callbacks=[lr_find] ) plt.plot(lr_find.lrs, lr_find.losses) plt.xscale(&#39;log&#39;) plt.show() . 400/400 [==============================] - 2s 4ms/step - loss: 1.7651 - accuracy: 0.4492 . So looking at the plot above, the minimum occurs at 0.1, however this is most likely going to be unstable. So a good learning rate to use would be 0.01. . Shameless Self Promotion . I have a Machine Learning (and Deep Learning) course on Udemy. If you use the code DEEPSCHOOL2021 you can get the course for $15 instead of the usual $99. .",
            "url": "https://sachinruk.github.io/blog/tensorflow/learning%20rate/2021/02/15/Tensorflow-Learning-Rate-Finder.html",
            "relUrl": "/tensorflow/learning%20rate/2021/02/15/Tensorflow-Learning-Rate-Finder.html",
            "date": " ‚Ä¢ Feb 15, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Focal Loss for Multi-class Classification",
            "content": "class WeightedFocalLoss(nn.Module): &quot;Non weighted version of Focal Loss&quot; def __init__(self, weights, gamma=1.1): super().__init__() self.weights = weights self.gamma = gamma def forward(self, inputs, targets): inputs = inputs.squeeze() targets = targets.squeeze() BCE_loss = F.cross_entropy(inputs, targets, reduction=&#39;none&#39;) pt = torch.exp(-BCE_loss) F_loss = self.weights[targets]*(1-pt)**self.gamma * BCE_loss return F_loss.mean() .",
            "url": "https://sachinruk.github.io/blog/pytorch/loss%20function/2020/11/28/focal-loss.html",
            "relUrl": "/pytorch/loss%20function/2020/11/28/focal-loss.html",
            "date": " ‚Ä¢ Nov 28, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Docker for Data Science",
            "content": "Docker for Data Science . Docker is a tool that simplifies the installation process for software engineers. Coming from a statistics background I used to care very little about how to install software and would occasionally spend a few days trying to resolve system configuration issues. Enter the god-send Docker almighty. . Think of Docker as a light virtual machine (I apologise to the Docker gurus for using that term). Its underlying philosophy is that if it works on my machine it will work on yours. . What‚Äôs in it for Data Scientists . Time: The amount of time that you save on not installing packages in itself makes this framework worth it. | Reproducible Research: I think of Docker as akin to setting the seed in a report. This makes sure that the analysis that you are generating will run on any other analysts machine. | How Does it Work? . Docker employs the concept of (reusable) layers. So whatever line that you write inside the Dockerfile is considered a layer. For example you would usually start with: . FROM ubuntu RUN apt-get install python3 . This Dockerfile would install python3 (as a layer) on top of the Ubuntu layer. . What you essentially do is for each project you write all the apt-get install, pip install etc. commands into your Dockerfile instead of executing it locally. . I recommend reading the tutorial on https://docs.docker.com/get-started/ to get started on Docker. The learning curve is minimal (2 days work at most) and the gains are enormous. . Dockerhub . Lastly Dockerhub deserves a special mention. Personally Dockerhub is what makes Docker truly powerful. It‚Äôs what github is to git, a open platform to share your Docker images. . My Docker image for Machine Learning and data science is availale here: https://hub.docker.com/r/sachinruk/ml_class/ .",
            "url": "https://sachinruk.github.io/blog/blog/2017/08/24/Docker-for-Data-Science.html",
            "relUrl": "/blog/2017/08/24/Docker-for-Data-Science.html",
            "date": " ‚Ä¢ Aug 24, 2017"
        }
        
    
  
    
        ,"post7": {
            "title": "DeepSchool.io",
            "content": "DeepSchool.io . NodeSchool is one of the most inclusive software communities that I have come across. What I liked about it the most is its emphasis on writing code. There are so many meetups that I have been to where I simply listen to talks and go home without much of a takehome message. . www.DeepSchool.io is an open-source, community based project to teach the A-Z of Deep Learning (DL). This project came out of a weekly class that I did at Arbor Networks where I work as a Data Scientist. . Personally I come from a background where I did a PhD in Machine Learning. However, with the development of tools such as Keras, DL has become a lot more accessible to the general community. . Even with these available tools teaching Deep Learning can be quite difficult. The first lesson I did was a complete train wreck. I had forgotten where I started and jumped straight into a multi layered Deep Net. I Took for granted that people would understand what a loss function is, and what regression vs logisitic regression is. . Conversely I did not want to spend too much time on the mathematics either. I wanted to create something that would get people tackling DL problems fast instead of diving too deep into the theory. I spent 6 months or so on Andrew Ngs DL course that did go through the theory. This unfortunately did not equip me with the tools necessary towards actually being comfortable with using DL in any meaningful way. The goal is to focus on the bigger picture of what you can do with DL. . Goals . Make Deep Learning easier (minimal code). | Minimise required mathematics. | Make it practical (runs on laptops). | Open Source Deep Learning Learning. | Grow a collaborating practical community around DL. | The assumed knowledge is that you are able to code in Python. I make all code available in Jupyter Notebooks for the sole reason being that you can interact with it. Running on a single python script decreases this interactivity. . I also use Docker containers along with Docker-compose so that I don‚Äôt have to deal with installation issues. This tends to take up upwards of half an hour at some workshops. Mind you, the current container that I have put up uses 3GB of space. . Call for Contributions . There is still much to do with Deep School. These are some of the most important requirements in order of importance: . Use the tutorials! | Help with documenting tutorials (there are parts I could have explained better). | Contribute tutorials. At the time of writing I am yet to do a LSTM tutorial. Furthermore I am yet to provide the more advanced tutorials such as Attention Networks, Generative Adversarial Networks, Reinforcement Learning etc. | Help me setup a website/ forum. I have limited experience with websites. It would be good to provide a NodeSchool.io style webpage so that we could spread the message. |",
            "url": "https://sachinruk.github.io/blog/blog/2017/07/04/DeepSchool.io.html",
            "relUrl": "/blog/2017/07/04/DeepSchool.io.html",
            "date": " ‚Ä¢ Jul 4, 2017"
        }
        
    
  
    
        ,"post8": {
            "title": "Keras LSTMs",
            "content": "Keras has been one of the really powerful Deep Learning libraries that allow you to have a Deep Net running in a few lines of codes. Best part, don‚Äôt worry about the math. In the following videos you will find how to implement a popular Recursive Neural Net (RNN) called Long Short Term Memory RNNs (LSTM). . Note: You could easily replace the LSTM units with Gated Recurrent Units (GRU) with the same function call. . Source code: https://github.com/sachinruk/PyData_Keras_Talk/blob/master/cosine_LSTM.ipynb . FAQ: . Why do we need a Dense Layer? The output is still one dimensional (y) and therefore the 32 hidden layers need to be projected down to one. Hence the dense layer is used. | How do you decide number of layers and number of nodes in each layer? Personally for me this is trial and error. Generally larger number of layers (deeper) is better than going wide (more nodes). But I usually limit myself to 5 at most unless there is a truly large dataset (100MB+) | References . To understand the maths behind LSTM: http://colah.github.io/posts/2015-08-Understanding-LSTMs/ | For another guide to Keras LSTMs: http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/ | If you are still confused (try my stackoverflow post): http://stackoverflow.com/questions/38714959/understanding-keras-lstms |",
            "url": "https://sachinruk.github.io/blog/blog/2016/10/20/Keras-LSTM.html",
            "relUrl": "/blog/2016/10/20/Keras-LSTM.html",
            "date": " ‚Ä¢ Oct 20, 2016"
        }
        
    
  
    
        ,"post9": {
            "title": "Deep Learning Quantile Regression -  Keras",
            "content": "The loss function is simple as doing the following. Which is simply the pin-ball loss function. . def tilted_loss(q,y,f): e = (y-f) return K.mean(K.maximum(q*e, (q-1)*e), axis=-1) . When it comes to compiling the neural network, just simply do: . model.compile(loss=lambda y,f: tilted_loss(0.5,y,f), optimizer=&#39;adagrad&#39;) . I chose 0.5 which is the median, but you can try whichever quantile that you are after. Word of caution, which applies to any quantile regression method; you may find that the quantile output might be extreme/ unexpected when you take extreme quantiles (eg. 0.001 or 0.999). . A more complete working example can be found here. .",
            "url": "https://sachinruk.github.io/blog/blog/2016/10/16/Quantile-Regression.html",
            "relUrl": "/blog/2016/10/16/Quantile-Regression.html",
            "date": " ‚Ä¢ Oct 16, 2016"
        }
        
    
  
    
        ,"post10": {
            "title": "XgBoost - Machine Learning made EASY!",
            "content": "One of the machine learning frameworks that has been exploding on the Kaggle scene has been Xgboost. In my personal experience it has been an extremely powerful machine learning algorithm, beating random forests on most problems I‚Äôve played around with. . The following video is a quick introduction to XgBoost. .",
            "url": "https://sachinruk.github.io/blog/blog/2016/08/08/XgBoost.html",
            "relUrl": "/blog/2016/08/08/XgBoost.html",
            "date": " ‚Ä¢ Aug 8, 2016"
        }
        
    
  
    
        ,"post11": {
            "title": "Reversible jump MCMC",
            "content": "Reversible jump MCMC . Reversible jump MCMC is a Bayesian algorithm to infer the number of components/ clusters from a set of data. For this illustration we shall consider a two component model at most. . Model . The likelihoods can be represented as: begin{align} p(y_i| lambda_{11},k=1)=&amp; lambda_{11} exp(- lambda_{11}y_i) p(y_i| lambda_{12}, lambda_{22},k=2,z_i)=&amp; prod_j ( lambda_{j2} exp(- lambda_{j2}y_i))^{1(z_i=j)} end{align} . The priors on the latent variables are: . begin{align} p( lambda_{jk}) propto &amp; frac{1}{ lambda_{jk}} qquad lambda_{jk} in[a,b] p(z_i=1)=&amp; pi p( pi) = &amp; text{Dir}( alpha) p(k=j)= &amp; 1/K end{align} . Jumping dimensions . We need to consider a Metropolis-Hastings (MH) step to consider going from one component to two components. The MH step in general is as follows: . begin{align} alpha = &amp; frac{p(y, theta_2^{t+1})}{p(y, theta_1^t)} frac{q( theta_1^t| theta_2^{t+1})}{q( theta_2^{t+1}| theta_1^{t})} A = &amp; text{min} left(1, alpha right) end{align} . where, . begin{align} p(y_i, theta_2)=&amp; p(y| lambda_{12}, lambda_{22}, pi)p( lambda_{12})p( lambda_{22})p( pi) =&amp; pi p(y_i| lambda_{12})+(1- pi) p(y_i| lambda_{22}) end{align} . Jumping from 1 dim to 2 . In this case let the parameters Œ∏={‚à™jŒªjk,k,œÄ} theta= { cup_j lambda_{jk},k, pi }Œ∏={‚à™j‚ÄãŒªjk‚Äã,k,œÄ} . As we can let the proposal distribution be anything, we let q(Œ∏1‚ÜíŒ∏2)q( theta_1 to theta_2)q(Œ∏1‚Äã‚ÜíŒ∏2‚Äã) as follows: begin{align} q( lambda_{j2}, pi,k=2|k=1, lambda_{11})=q( lambda_{j2}|k=2, lambda_{11})q( pi|k=2)q(k=2|k=1) end{align} . We let the proposal q(k=2‚à£k=1)=1q(k=2 vert k=1)=1q(k=2‚à£k=1)=1. We also have the following dimensional jump: . begin{align} mu_1, mu_2 sim &amp; U(0,1) lambda_{12}=&amp; lambda_{11} frac{ mu_1}{1- mu_1} lambda_{22}=&amp; lambda_{11} frac{1- mu_1}{ mu_1} pi=&amp; mu_2 end{align} . Thus, in order to find the distribution (q( lambda_{j2} vert k=2, lambda_{11})) we use the change of variable identity that q(Œªj2‚à£k=2,Œª11)=q(Œº1)‚à£J‚à£q( lambda_{j2} vert k=2, lambda_{11})=q( mu_1) vert J vertq(Œªj2‚Äã‚à£k=2,Œª11‚Äã)=q(Œº1‚Äã)‚à£J‚à£ where, JJJ is the jacobian ‚àÇ(Œª11,Œº1)‚àÇ(Œª12,Œª22) frac{ partial( lambda_{11}, mu_1)}{ partial( lambda_{12}, lambda_{22})}‚àÇ(Œª12‚Äã,Œª22‚Äã)‚àÇ(Œª11‚Äã,Œº1‚Äã)‚Äã. The Jacobian determinant is found to be Œº1(1‚àíŒº1)2Œª11 frac{ mu_1(1- mu_1)}{2 lambda_{11}}2Œª11‚ÄãŒº1‚Äã(1‚àíŒº1‚Äã)‚Äã while q(Œº1)=q(Œº2)=1q( mu_1)=q( mu_2)=1q(Œº1‚Äã)=q(Œº2‚Äã)=1 since they are sampled from standard uniform distributions. Also (q( mu_2)=q( pi vert k=2)). . Since we need the ratio of proposed states ( frac{q( theta_1^t | theta_2^{t+1})}{q( theta_2^{t+1} | theta_1^{t})} ) we are also required to find ( q( lambda_{11},k=2 vert lambda_{2j}, pi,k=1) = q( lambda_{11} vert lambda_{2j},k=2) q(k=1 vert k=2) ). We again take ( q(k=1 vert k=2)=1 ). (q( lambda_{11}= sqrt{ lambda_{12} lambda_{22}})=1) | . Jumping from 2 to 1 . The MH step is conducted using the reciprocal of $ alpha$ in the equation above. . RJMCMC Algorithm .",
            "url": "https://sachinruk.github.io/blog/blog/2015/10/20/Reversible-Jump-MCMC.html",
            "relUrl": "/blog/2015/10/20/Reversible-Jump-MCMC.html",
            "date": " ‚Ä¢ Oct 20, 2015"
        }
        
    
  
    
        ,"post12": {
            "title": "Chinese Restuarant Process",
            "content": "In this instance we generate the parameters Œ∏k theta_kŒ∏k‚Äã from N(0,3I) mathcal{N}( mathbf{0},3 mathbf{I})N(0,3I). The data is generated from N(Œ∏k,0.1I) mathcal{N}( theta_k,0.1 mathbf{I})N(Œ∏k‚Äã,0.1I). Where kkk is the table. Table allocation is the main part of the CRP which is determined by: begin{align} k= begin{cases} text{new table } &amp; text{with prob = } frac{ alpha}{ alpha+n-1} text{table k } &amp; text{with prob = } frac{n_k}{ alpha+n-1} end{cases} end{align} where nkn_knk‚Äã is the number of customers at table kkk. . The associated ipython notebook is located here. .",
            "url": "https://sachinruk.github.io/blog/blog/2015/10/09/Chinese-Restaurant-Process.html",
            "relUrl": "/blog/2015/10/09/Chinese-Restaurant-Process.html",
            "date": " ‚Ä¢ Oct 9, 2015"
        }
        
    
  
    
        ,"post13": {
            "title": "von Mises-Fisher Distribution",
            "content": "The von Mises Fisher Distribution is a multivariate distribution on a hyper sphere. I have decided to share the expectation and covariance of the vMF distribution. The Wikipedia page doesn‚Äôt give much info of this distribution. . Expectation of vMF distribution . Let (C ) be the normalising constant. . ‚à´‚à£‚à£x‚à£‚à£2=1exp‚Å°(Œ∫ŒºTx)‚Äâdx=(2œÄ)d/2‚àí1Id/2‚àí1(Œ∫)Œ∫d/2‚àí1=C int_{|| mathbf{x}||_2=1} exp( kappa mathbf{ mu}^T mathbf{x}) ,d mathbf{x} = frac{(2 pi)^{d/2-1} I_{d/2-1}( kappa)}{ kappa^{d/2-1}}=C‚à´‚à£‚à£x‚à£‚à£2‚Äã=1‚Äãexp(Œ∫ŒºTx)dx=Œ∫d/2‚àí1(2œÄ)d/2‚àí1Id/2‚àí1‚Äã(Œ∫)‚Äã=C . Let ( mathbf{y}= kappa mathbf{ mu} ). Therefore ( kappa= sqrt{ mathbf{y}^T mathbf{y}} ). . begin{align} frac{d kappa}{d mathbf{y}}= frac{1}{2} frac{ mathbf{y}}{ sqrt{ mathbf{y}^T mathbf{y}}}= frac{ kappa mathbf{ mu}}{ kappa}= mathbf{ mu} end{align} . begin{align} int mathbf{x} exp( mathbf{y}^T mathbf{x}) d mathbf{x} =&amp; frac{d}{d mathbf{y}} int exp( mathbf{y}^T mathbf{x}) d mathbf{x} =&amp; frac{d kappa}{d mathbf{y}} frac{d}{d kappa} int exp( mathbf{y}^T mathbf{x}) d mathbf{x} =&amp; mathbf{ mu} frac{d}{d kappa} frac{(2 pi)^{d/2-1} I_{d/2-1}( kappa)}{ kappa^{d/2-1}} =&amp; mathbf{ mu} left( frac{I&#39;{d/2-1}( kappa)}{I{d/2-1}( kappa)} - frac{d/2-1}{ kappa} right) frac{(2 pi)^{d/2-1} I_{d/2-1}( kappa)}{ kappa^{d/2-1}} E( mathbf{x}) =&amp; frac{ int mathbf{x} exp( mathbf{y}^T mathbf{x}) d mathbf{x}}{ int exp( mathbf{y}^T mathbf{x}) d mathbf{x}} = mathbf{ mu} left( frac{I&#39;{d/2-1}( kappa)}{I{d/2-1}( kappa)} - frac{d/2-1}{ kappa} right) E( mathbf{x}) =&amp; frac{I_{d/2}( kappa)}{I_{d/2-1}( kappa)} mathbf{ mu} end{align} . This is an interesting result because its saying that the mean of a von Mises-Fisher distribution is NOT ( mathbf{ mu} ). It is infact multiplied a constant Id/2(Œ∫)Id/2‚àí1(Œ∫) frac{I_{d/2}( kappa)}{I_{d/2-1}( kappa)}Id/2‚àí1‚Äã(Œ∫)Id/2‚Äã(Œ∫)‚Äã which is between ((0,1) ). If you think about a uniformly distributed vMF this makes sense ( ( kappa to 0 )). If we average all those vectors pointing in different directions it averages very close to 0. This whole ‚Äòaveraging‚Äô of unit vectors is what makes the expected value not equal ( mathbf{ mu} ) but a vector pointing in the same direction but smaller in length. . ##Covariance of von Mises-Fisher Distribution . Using the same differential approach we can find E(xxT)E( mathbf{xx}^T)E(xxT) and hence the covariance by using the identity cov(x,x)=E(xxT)‚àíE(x)E(x)Tcov( mathbf{x}, mathbf{x})=E( mathbf{xx}^T)-E( mathbf{x})E( mathbf{x})^Tcov(x,x)=E(xxT)‚àíE(x)E(x)T. Hence the covariance is, . begin{align} frac{h( kappa)}{ kappa} mathbf{I}+ left(1-2 frac{ nu+1}{ kappa}h( kappa)-h( kappa)^2 right) mathbf{ mu} mathbf{ mu}^T end{align} . where h(Œ∫)=IŒΩ+1(Œ∫)IŒΩ(Œ∫)h( kappa)= frac{I_{ nu+1}( kappa)}{I_{ nu}( kappa)}h(Œ∫)=IŒΩ‚Äã(Œ∫)IŒΩ+1‚Äã(Œ∫)‚Äã and ŒΩ=d/2‚àí1 nu=d/2-1ŒΩ=d/2‚àí1. .",
            "url": "https://sachinruk.github.io/blog/blog/2015/08/10/von-Mises-Fisher.html",
            "relUrl": "/blog/2015/08/10/von-Mises-Fisher.html",
            "date": " ‚Ä¢ Aug 10, 2015"
        }
        
    
  
    
        ,"post14": {
            "title": "Sample Variance",
            "content": "People often question why is there a ‚Äún-1‚Äù term when I calculate the variance. Why not divide through by ‚Äún‚Äù. Most stats courses dismiss this question by saying, ‚Äúoh, that‚Äôs because you lose a degree of freedom‚Äù. What is a degree of freedom. In the video below we ignore this notion of degree of freedom and answer where the ‚Äún-1‚Äù came from when calculating sample variance. .",
            "url": "https://sachinruk.github.io/blog/blog/2015/08/06/Sample-Variance.html",
            "relUrl": "/blog/2015/08/06/Sample-Variance.html",
            "date": " ‚Ä¢ Aug 6, 2015"
        }
        
    
  
    
        ,"post15": {
            "title": "Normal Distribution",
            "content": "No stats blog would be complete without a discussion of the Gaussian distribution. In the following video I discuss how to obtain the mean and variance of a Gaussian. You do need some knowledge of integration. . Oh, and this is a statisticians ‚Äúhello world‚Äù. .",
            "url": "https://sachinruk.github.io/blog/blog/2015/08/02/Normal-Distribution.html",
            "relUrl": "/blog/2015/08/02/Normal-Distribution.html",
            "date": " ‚Ä¢ Aug 2, 2015"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, everyone. And thanks for stopping by. My name is Sachin Abeywardana and currently a Machine Learning Engineer at Canva. . I did my PhD in Bayesian Machine Learning but every since I left have been obsessed about the wonderful world of Deep Learning. These days I‚Äôm working on bringing NLP goodness into Canva. . I have created a Udemy course on Machine Learning and can be found here. Feel free to ping me for a coupon. .",
          "url": "https://sachinruk.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://sachinruk.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}